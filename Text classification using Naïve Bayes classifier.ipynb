{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "- The objective from this project is to create a <b>Spam filter using NaiveBayes classifier</b>.\n",
    "- We'll use a dataset from UCI Repository. SMS Spam Detection: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUxZnsqrmynW"
   },
   "source": [
    "### Create a spark session and import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# from pyspark import  SparkContext\n",
    "# sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUxZnsqrmynW"
   },
   "source": [
    "### Read the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|ham\tGo until juro...|\n",
      "|ham\tOk lar... Jok...|\n",
      "|spam\tFree entry i...|\n",
      "|ham\tU dun say so ...|\n",
      "|ham\tNah I don't t...|\n",
      "|spam\tFreeMsg Hey ...|\n",
      "|ham\tEven my broth...|\n",
      "|ham\tAs per your r...|\n",
      "|spam\tWINNER!! As ...|\n",
      "|spam\tHad your mob...|\n",
      "|ham\tI'm gonna be ...|\n",
      "|spam\tSIX chances ...|\n",
      "|spam\tURGENT! You ...|\n",
      "|ham\tI've been sea...|\n",
      "|ham\tI HAVE A DATE...|\n",
      "|spam\tXXXMobileMov...|\n",
      "|ham\tOh k...i'm wa...|\n",
      "|ham\tEh u remember...|\n",
      "|ham\tFine if that...|\n",
      "|spam\tEngland v Ma...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.text('SMSSpamCollection')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = df.toPandas()\n",
    "\n",
    "data = [ (i[:3], i[3:]) for i in pd_df['value'] ]\n",
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUxZnsqrmynW"
   },
   "source": [
    "### Print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| _1|                  _2|\n",
      "+---+--------------------+\n",
      "|ham|\tGo until jurong ...|\n",
      "|ham|\tOk lar... Joking...|\n",
      "+---+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUxZnsqrmynW"
   },
   "source": [
    "### Rename the first column to 'class' and second column to 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('_1', 'class')\n",
    "df = df.withColumnRenamed('_2', 'text') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|\tGo until jurong ...|\n",
      "|  ham|\tOk lar... Joking...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the first 10 rows from the dataframe\n",
    "- Show once with truncate=True and once with truncate=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|\tGo until jurong ...|\n",
      "|  ham|\tOk lar... Joking...|\n",
      "|  spa|m\tFree entry in 2...|\n",
      "|  ham|\tU dun say so ear...|\n",
      "|  ham|\tNah I don't thin...|\n",
      "|  spa|m\tFreeMsg Hey the...|\n",
      "|  ham|\tEven my brother ...|\n",
      "|  ham|\tAs per your requ...|\n",
      "|  spa|m\tWINNER!! As a v...|\n",
      "|  spa|m\tHad your mobile...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|class|text                                                                                                                                                             |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ham  |\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                                 |\n",
      "|ham  |\tOk lar... Joking wif u oni...                                                                                                                                   |\n",
      "|spa  |m\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's    |\n",
      "|ham  |\tU dun say so early hor... U c already then say...                                                                                                               |\n",
      "|ham  |\tNah I don't think he goes to usf, he lives around here though                                                                                                   |\n",
      "|spa  |m\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv            |\n",
      "|ham  |\tEven my brother is not like to speak with me. They treat me like aids patent.                                                                                   |\n",
      "|ham  |\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune|\n",
      "|spa  |m\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.  |\n",
      "|spa  |m\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030     |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new feature column contains the length of the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "# df.withColumn('text_len', col('text').count(\"+\"))\n",
    "# # df.withColumn('text_len', col('text').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import count\n",
    "\n",
    "all_text = df.select('text').collect()\n",
    "lst = []\n",
    "\n",
    "for i in range(0, len(all_text)): \n",
    "    lst.append(len(all_text[i][0].strip('\\n\\t')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('length', length('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|\tGo until jurong ...|   112|\n",
      "|  ham|\tOk lar... Joking...|    30|\n",
      "|  spa|m\tFree entry in 2...|   157|\n",
      "|  ham|\tU dun say so ear...|    50|\n",
      "|  ham|\tNah I don't thin...|    62|\n",
      "|  spa|m\tFreeMsg Hey the...|   149|\n",
      "|  ham|\tEven my brother ...|    78|\n",
      "|  ham|\tAs per your requ...|   161|\n",
      "|  spa|m\tWINNER!! As a v...|   159|\n",
      "|  spa|m\tHad your mobile...|   156|\n",
      "|  ham|\tI'm gonna be hom...|   110|\n",
      "|  spa|m\tSIX chances to ...|   138|\n",
      "|  spa|m\tURGENT! You hav...|   157|\n",
      "|  ham|\tI've been search...|   197|\n",
      "|  ham|\tI HAVE A DATE ON...|    36|\n",
      "|  spa|m\tXXXMobileMovieC...|   151|\n",
      "|  ham|\tOh k...i'm watch...|    27|\n",
      "|  ham|\tEh u remember ho...|    82|\n",
      "|  ham|\tFine if thats t...|    57|\n",
      "|  spa|m\tEngland v Maced...|   157|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the average text length for each class (give alias name to the average length column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|          Average|\n",
      "+-----+-----------------+\n",
      "|  ham|72.47192873420344|\n",
      "|  spa|140.6760374832664|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.groupBy('class').agg(avg('length').alias('Average')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this part you transform your raw text in to tf_idf model :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the following steps to obtain TF-IDF:\n",
    "1. Import the required transformers/estimators for the subsequent steps.\n",
    "2. Create a <b>Tokenizer</b> from the text column.\n",
    "3. Create a <b>StopWordsRemover</b> to remove the <b>stop words</b> from the column obtained from the <b>Tokenizer</b>.\n",
    "4. Create a <b>CountVectorizer</b> after removing the <b>stop words</b>.\n",
    "5. Create the <b>TF-IDF</b> from the <b>CountVectorizer</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='text_tokenized')\n",
    "SWRemover = StopWordsRemover(inputCol='text_tokenized', outputCol='text_no_sw')\n",
    "countvec = CountVectorizer(inputCol='text_no_sw', outputCol='text_cv')\n",
    "tf_idf = IDF(inputCol='text_cv', outputCol='tf_idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert the <b>class column</b> to index using <b>StringIndexer</b>\n",
    "- Create feature column from the <b>TF-IDF</b> and <b>lenght</b> columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder,VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "strIndexer = StringIndexer(inputCol = 'class', outputCol = 'indexed_class')\n",
    "\n",
    "all_cols = ['tf_idf'] + ['length']\n",
    "vecassembler = VectorAssembler(inputCols = all_cols, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "- Create a <b>NaiveBayes</b> classifier with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "NBC = NaiveBayes(featuresCol='features',labelCol='indexed_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "### Create a pipeline model contains all the steps starting from the Tokenizer to the NaiveBays classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer,\n",
    "                            SWRemover,\n",
    "                            countvec,\n",
    "                            tf_idf,\n",
    "                            strIndexer, vecassembler, NBC])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split your data to trian and test data with ratios 0.7 and 0.3 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([.7,.3],seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit your Pipeline model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform predictions on tests dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the schema of the prediction dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- text_tokenized: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- text_no_sw: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- text_cv: vector (nullable = true)\n",
      " |-- tf_idf: vector (nullable = true)\n",
      " |-- indexed_class: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "- Use <b>MulticlassClassificationEvaluator</b> to calculate the <b>f1_score</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790754024460486"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "f1_score = BinaryClassificationEvaluator(predictionCol='prediction' ,labelCol='indexed_class', metricName='f1')\n",
    "f1_score.evaluate(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
